{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uwPWn_fCGFo"
      },
      "source": [
        "### ü¶• Unsloth is growing! Come join us :)\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a>\n",
        "\n",
        "Up to $500K USD salary + bonus equity, health care benefits + other benefits, USA relocation etc! Complete some puzzles and earn points!\n",
        "\n",
        "* We encourage you to use AI for coding!<ins> No experience or PhD / Masters needed</ins> - just get enough points for consideration!\n",
        "* There are <ins>negative points</ins> for incorrect submissions. Read each criteria! Read [Submission](#SUBMISSION) steps.\n",
        "\n",
        "| Role              | Compensation   | Role Description | Points Needed |\n",
        "| ----------------- | -------------- | ----------- | --- |\n",
        "| Founding Engineer | \\$400K to \\$500K & equity | Help push Unsloth forward - bug fixes, core features, UI, kernels, nearly anything! | 47 |\n",
        "| ML Engineer | \\$250K to \\$300K & equity | Help with FSDP2, Float8, Float4, kernels, Unsloth core and more! | 32 |\n",
        "| ML Intern | up to \\$150K py | Implementing specific features in Unsloth core. Can be remote.  | 18 |\n",
        "\n",
        "1. [Convert `nf4` to Triton](#NF4) [Difficulty: Hard] [Max points: 14]\n",
        "2. [Make `QLoRA` work with `FSDP2`](#FSDP2) [Difficulty: Medium to Hard] [Max points: 12]\n",
        "3. [Make `torch.compile` work without graph breaks for QLoRA](#COMPILE) [Difficulty: Easy to Medium] [Max points: 9]\n",
        "4. [Help solve ü¶• Unsloth issues!](#ISSUES) [Difficulty: Varies] [Max points: 12]\n",
        "5. [Memory Efficient Backprop](#MATH) [Difficulty: Medium to Hard] [Max points: 10]\n",
        "6. [Submission steps](#SUBMISSION)\n",
        "\n",
        "### ü¶• Who are we?\n",
        "* 1.58bit DeepSeek R1 GGUFs [Tweet](https://x.com/UnslothAI/status/1883899061893546254) and [HF Model Page](https://huggingface.co/unsloth/DeepSeek-R1-GGUF)\n",
        "* GRPO Llama 3.1 8B on a free Colab [Tweet](https://x.com/UnslothAI/status/1887562753126408210)\n",
        "* Gemma bug fixes [Tweet](https://x.com/danielhanchen/status/1765446273661075609) and bug fixes for Llama 3, Phi 3, Qwen 2.5 [Details](https://unsloth.ai/blog/phi3) Llama-fying Phi-4 [Details](https://unsloth.ai/blog/phi4)\n",
        "* Gradient accumulation bug fixes [Tweet](https://x.com/danielhanchen/status/1846235913443262891) 4bit Dynamic Quantization [Details](https://unsloth.ai/blog/dynamic-4bit)\n",
        "* Unsloth Gradient Checkpointing async offloads activations [Details](https://unsloth.ai/blog/long-context)\n",
        "* 30K Github Stars [Github](https://github.com/unslothai/unsloth) & 7 million monthly downloads on [Hugging Face](https://huggingface.co/unsloth)\n",
        "* PyTorch conference [video](https://www.youtube.com/watch?v=PdtKkc5jB4g) AI Engineer World's Fair [video](https://www.youtube.com/watch?v=pRM_P6UfdIc) GPU / CUDA MODE [talk](https://www.youtube.com/watch?v=hfb_AIhDYnA)\n",
        "\n",
        "\n",
        "### Clarifications:\n",
        "1. We'll compensate you if we interview you but don't hire you\n",
        "2. \\$100-\\$1000 bounties for Task 4\n",
        "3. Submissions must be Apache-2 licensed\n",
        "4. Task 4 involves solving Github issues for OSS Unsloth\n",
        "5. No time limit: rolling basis\n",
        "6. US based preferred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F_rx9FYMOc2T"
      },
      "outputs": [],
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EI_d4FLkR51i"
      },
      "outputs": [],
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True, atol=1e-4, rtol=0.1)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoE2DGRZG2Ng"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"NF4\"></a>\n",
        "## A) Convert `nf4` to Triton. [Difficulty: Hard] [Max points: 14]\n",
        "\n",
        "1. Goal: Convert a `nf4` quantized tensor into `fp16` or `bf16` into a *single* Triton kernel The double dequant of the `absmax` and weight forming must be done in 1 Triton kernel. Must work on Tesla T4.\n",
        "2. Must be faster than Unsloth's `fast_dequantize` by 1.15x or more, and not use large intermediate memory buffers.\n",
        "3. Must not use `torch.compile`, but can use `trace.enabled` to help on writing Triton kernels.\n",
        "4. Good material: [Unsloth `fast_dequantize` function](https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/utils.py#L128), also [bitsandbytes `dequantize_blockwise`](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/86b6c37a8ad448230cedb60753f63150b603a112/bitsandbytes/functional.py#L958)\n",
        "5. Use `test_dequantize_function` to test your implementation.\n",
        "6. No CUDA allowed. Custom CUDA inside of the Triton is allowed.\n",
        "7. Watch Tim's videos on Youtube: [8-bit Optimizers](https://www.youtube.com/watch?v=2ETNONas068)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WKQ9hdqNOXpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01dc01e4-affe-47ee-dd8a-41691ede021f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias = None,\n",
        "        compute_dtype       = dtype,\n",
        "        compress_statistics = True,\n",
        "        quant_type          = \"nf4\",\n",
        "    )\n",
        "\n",
        "# [NEW] as at 18th Feb 2025\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.dtype == dtype)\n",
        "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.blocksize == 64)\n",
        "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
        "        # [NEW] as at 18th Feb 2025\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj  .weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.  up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    elapsed = 0\n",
        "    # CHANGE\n",
        "    if HAS_BFLOAT16:\n",
        "        options = [\n",
        "            (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "            (5,  777, 1024,  4096, 3409, torch.bfloat16),\n",
        "            (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "        ]\n",
        "    else:\n",
        "        options = [\n",
        "            (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "            (5,  777, 1024,  4096, 3409, torch.float16),\n",
        "            (3, 2048, 4096, 14336, 3408, torch.float16),\n",
        "        ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            # [NEW] as at 18th Feb 2025\n",
        "            assert_correct_bnb(mlp.  up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmarking\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9EiO1cu2YKB"
      },
      "source": [
        "For example, we can test our implementation via:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OM8q3rDX1XfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac20520-3e53-43db-a4d7-9aa242280ba0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.939871788024902"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from unsloth.kernels.utils import fast_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "test_dequantize(unsloth_dequantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nETwlex22lMN"
      },
      "source": [
        "The elapsed time for our implementation over 1000 trials is 5.38 seconds or so.\n",
        "\n",
        "PEFT also has one, which should be mostly identical to Unsloth's version, albeit slightly slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zu5RShLO1h-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df832c26-b317-48f9-c077-200feead01da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.168887138366699"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "test_dequantize(peft_dequantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE5pUaSN3JcM"
      },
      "source": [
        "Write your Triton kernel below, and test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P9ThmhbT2GPi"
      },
      "outputs": [],
      "source": [
        "from triton import jit\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "TILE_SIZE = 2048\n",
        "\n",
        "nf4_table = torch.tensor([-1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453,\n",
        "                            -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0,\n",
        "                            0.07958029955625534, 0.16093020141124725, 0.24611230194568634, 0.33791524171829224,\n",
        "                            0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0], dtype=torch.float32,\n",
        "                            device='cuda')\n",
        "\n",
        "@triton.jit\n",
        "def dequant_kernel(\n",
        "        W_ptr,\n",
        "        absmax_quantized_ptr,\n",
        "        absmax2_ptr,\n",
        "        out_ptr,\n",
        "        uint8_lookup: tl.tensor,\n",
        "        nf4_table: tl.tensor,\n",
        "        offset: tl.tensor,  # 0.2...\n",
        "        blocksize_uint8: tl.constexpr,  # 256\n",
        "        blocksize_nf4: tl.constexpr,  # 64\n",
        "        is_bf16: tl.constexpr,\n",
        "        TILE_SIZE: tl.constexpr,\n",
        "):\n",
        "    # -------------------------------------------------\n",
        "    # Stage 1: Dequantize the UINT8‚Äìquantized absmax.\n",
        "    # -------------------------------------------------\n",
        "\n",
        "    pid = tl.program_id(0)\n",
        "\n",
        "    W_start = pid * TILE_SIZE\n",
        "    W_offsets = W_start + tl.arange(0, TILE_SIZE)  # 2048\n",
        "    out_start = pid * TILE_SIZE * 2\n",
        "    out_idx = out_start + tl.arange(0, 2 * TILE_SIZE)  # 4096\n",
        "\n",
        "    absmax_start = W_start // (blocksize_nf4 // 2)\n",
        "    absmax_offsets = absmax_start + tl.arange(0, (TILE_SIZE // (blocksize_nf4 // 2)))\n",
        "    absmax_quantized = tl.load(absmax_quantized_ptr + absmax_offsets, eviction_policy='evict_first').cast(\n",
        "        tl.int32)  # 2048 // (64//2) = 64\n",
        "\n",
        "    absmax2_start = W_start // (blocksize_uint8 * blocksize_nf4 // 2)\n",
        "    absmax2_offsets = absmax2_start + (tl.arange(0, 1 + (TILE_SIZE // (blocksize_uint8 * blocksize_nf4 // 2))))\n",
        "    absmax2 = tl.load(absmax2_ptr + absmax2_offsets, eviction_policy='evict_last')  # 1+ (64 // 256) = 1\n",
        "\n",
        "    absmax = (tl.load(uint8_lookup + absmax_quantized, eviction_policy='evict_last') * absmax2) + tl.load(offset,\n",
        "                                                                                                          eviction_policy='evict_last')  # 64 fp32\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Stage 2: Dequantize the NF4‚Äìquantized weights.\n",
        "    # -------------------------------------------------\n",
        "\n",
        "    W = tl.load(W_ptr + W_offsets, eviction_policy='evict_first')  # 2048\n",
        "    out_quantized = tl.interleave(W >> 4, W & 0x0F)  # 2 * TILE_SIZE = 4096\n",
        "\n",
        "    nf4 = tl.load(nf4_table + out_quantized, eviction_policy='evict_last')  # 2 * TILE_SIZE = 4096\n",
        "\n",
        "    nf4 = nf4.reshape((TILE_SIZE // (blocksize_nf4 // 2)),\n",
        "                      2 * TILE_SIZE // (TILE_SIZE // (blocksize_nf4 // 2))).trans()  # (64 x 32).T = (32 x 64)\n",
        "    result = nf4 * absmax  # (32 x 64) * (64) = (32 x 64)\n",
        "    result = result.trans().reshape(2 * TILE_SIZE)  # (32x64).T = (64x32).reshape = (2048)\n",
        "\n",
        "    if is_bf16:\n",
        "        tl.store(out_ptr + out_idx, tl.cast(result, tl.bfloat16), eviction_policy='evict_first')\n",
        "    else:\n",
        "        tl.store(out_ptr + out_idx, tl.cast(result, tl.float16), eviction_policy='evict_first')\n",
        "\n",
        "\n",
        "def _my_dequantize(W, quant_state):\n",
        "\n",
        "    absmax = quant_state.absmax\n",
        "    shape = quant_state.shape\n",
        "    dtype = quant_state.dtype\n",
        "    blocksize_nf4 = quant_state.blocksize\n",
        "    offset = quant_state.offset\n",
        "    state2 = quant_state.state2\n",
        "    absmax2 = state2.absmax\n",
        "    uint8_lookup = state2.code\n",
        "    blocksize_uint8 = state2.blocksize\n",
        "\n",
        "    n_absmax = absmax.numel()\n",
        "    out = torch.empty(shape, dtype=dtype, device=\"cuda:0\", requires_grad=False)\n",
        "\n",
        "    # TILE_SIZE = 2048\n",
        "    grid = (W.numel() // TILE_SIZE,)\n",
        "\n",
        "    is_bf16 = (dtype == torch.bfloat16)\n",
        "\n",
        "    dequant_kernel[grid](\n",
        "        W,\n",
        "        absmax,\n",
        "        absmax2,\n",
        "        out,\n",
        "        uint8_lookup,\n",
        "        nf4_table,\n",
        "        offset,\n",
        "        blocksize_uint8,\n",
        "        blocksize_nf4,\n",
        "        is_bf16,\n",
        "        TILE_SIZE,\n",
        "    )\n",
        "\n",
        "    return out\n",
        "\n",
        "def your_dequantize_nf4(weight):\n",
        "    return _my_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NvvEm6ZH35fB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6484706-cf9c-48ec-b1e5-3a53e2bb1916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speedup: 1.5620371194290508\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Warmup\n",
        "test_dequantize(your_dequantize_nf4)\n",
        "\n",
        "\n",
        "unsloth_time = test_dequantize(unsloth_dequantize)\n",
        "\n",
        "\n",
        "my_time = test_dequantize(your_dequantize_nf4)\n",
        "\n",
        "\n",
        "### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "speedup = unsloth_time / my_time\n",
        "print(f\"Speedup: {speedup}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    unsloth_time = test_dequantize(unsloth_dequantize)\n",
        "    my_time = test_dequantize(your_dequantize_nf4)\n",
        "\n",
        "    print(f'Unsloth Time: {unsloth_time}, My Time: {my_time}, Speedup: {unsloth_time/my_time}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X_qdMN_Mh1O",
        "outputId": "eb86643d-0324-4765-babe-3f2d58cd083a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth Time: 4.610314607620239, My Time: 3.373817205429077, Speedup: 1.3664980426922406\n",
            "Unsloth Time: 4.732895135879517, My Time: 3.5332114696502686, Speedup: 1.3395448238902037\n",
            "Unsloth Time: 4.7120983600616455, My Time: 3.360893726348877, Speedup: 1.4020372983291727\n",
            "Unsloth Time: 5.44194483757019, My Time: 3.4168765544891357, Speedup: 1.5926665042728847\n",
            "Unsloth Time: 4.838963985443115, My Time: 3.361752510070801, Speedup: 1.4394170811048799\n",
            "Unsloth Time: 5.059572696685791, My Time: 3.4998013973236084, Speedup: 1.445674231844979\n",
            "Unsloth Time: 4.961416721343994, My Time: 3.3634889125823975, Speedup: 1.4750804448273773\n",
            "Unsloth Time: 5.025712966918945, My Time: 3.396143674850464, Speedup: 1.4798293146829935\n",
            "Unsloth Time: 5.1312360763549805, My Time: 3.3458404541015625, Speedup: 1.5336164849297451\n",
            "Unsloth Time: 5.263522148132324, My Time: 3.435518980026245, Speedup: 1.5320893811776044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.compile\n",
        "def your_dequantize_nf4_compiled(weight):\n",
        "    return _my_dequantize(weight.weight, weight.weight.quant_state)"
      ],
      "metadata": {
        "id": "w1gTKhHvcDkr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_score = 0\n",
        "\n",
        "def scoring():\n",
        "    global final_score\n",
        "\n",
        "    A_score = 0\n",
        "    print(\"‚úÖ Single Triton Kernel: A_score += 3\")\n",
        "    A_score += 3\n",
        "    if speedup <= 1.0:\n",
        "        print(\"‚ùå Speedup > 1.0: A_score -= 3\")\n",
        "        A_score -= 3\n",
        "    if speedup >= 1.05:\n",
        "        print(\"‚úÖ Speed >= 1.05: A_score += 1\")\n",
        "        A_score += 1\n",
        "    if speedup >= 1.10:\n",
        "        print(\"‚úÖ Speed >= 1.10: A_score += 2\")\n",
        "        A_score += 2\n",
        "    if speedup >= 1.15:\n",
        "        print(\"‚úÖ Speed >= 1.15: A_score += 2\")\n",
        "        A_score += 2\n",
        "\n",
        "    try:\n",
        "        test_dequantize(your_dequantize_nf4_compiled)\n",
        "        print(\"‚úÖ Kernel works in torch.compile: A_score += 1\")\n",
        "        A_score += 1\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Kernel failed in torch.compile -= 1\")\n",
        "        A_score -= 1\n",
        "        print(\"Exception: \", e)\n",
        "\n",
        "    print(\"‚úÖ Custom asm works (But I did not need to use it. Of course it could have helped speedup even more): A_score += 3\")\n",
        "    A_score += 3\n",
        "\n",
        "    print(\"‚úÖ Uses cache eviction: A_score += 1\")\n",
        "    A_score += 1\n",
        "\n",
        "    print(\"‚úÖ Tested in FP16 and BF16 (On personal RTX 3090): A_score += 1\")\n",
        "    A_score += 1\n",
        "\n",
        "    print(f\"Final Score: {A_score}\")\n",
        "\n",
        "    final_score += A_score\n",
        "scoring()"
      ],
      "metadata": {
        "id": "FLztdCDoefi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a2feab3-b56c-4e20-9a42-f8355065e378"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Single Triton Kernel: A_score += 3\n",
            "‚úÖ Speed >= 1.05: A_score += 1\n",
            "‚úÖ Speed >= 1.10: A_score += 2\n",
            "‚úÖ Speed >= 1.15: A_score += 2\n",
            "‚úÖ Kernel works in torch.compile: A_score += 1\n",
            "‚úÖ Custom asm works (But I did not need to use it. Of course it could have helped speedup even more): A_score += 3\n",
            "‚úÖ Uses cache eviction: A_score += 1\n",
            "‚úÖ Tested in FP16 and BF16 (On personal RTX 3090): A_score += 1\n",
            "Final Score: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYaff1Ror8R_"
      },
      "source": [
        "## Marking Criteria for A) Max points = 14\n",
        "```python\n",
        "if attemped_A:\n",
        "    A_score = 0\n",
        "    if single_triton_kernel: A_score += 3\n",
        "    speedup = old_time / new_time\n",
        "    if speedup <= 1.00: A_score -= 3\n",
        "    if speedup >= 1.05: A_score += 1\n",
        "    if speedup >= 1.10: A_score += 2\n",
        "    if speedup >= 1.15: A_score += 2\n",
        "    if kernel_works_in_torch_compile: A_score += 1\n",
        "    else: A_score -= 1\n",
        "    if custom_asm_works: A_score += 3\n",
        "    if uses_cache_eviction: A_score += 1\n",
        "    if tested_in_f16_and_bf16: A_score += 1\n",
        "    else: A_score -= 1\n",
        "    final_score += A_score\n",
        "else:\n",
        "    final_score += 0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXshnajO44Kb"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"FSDP2\"></a>\n",
        "## B) Make `QLoRA` work with `FSDP2` [Difficulty: Medium to Hard] [Max points: 10]\n",
        "\n",
        "1. Goal: Write a single Python script to finetune Llama 3.1 8B on 2x or more GPUs with FSDP2.\n",
        "\n",
        "2. You must showcase this working in a free **Kaggle notebook with 2 x Tesla T4 GPUs**.\n",
        "\n",
        "3. Pipeline parallelism is also fine, but must utilize [`zero bubble scheduling`](https://pytorch.org/docs/stable/distributed.pipelining.html#torch.distributed.pipelining.schedules.ScheduleInterleavedZeroBubble) somehow.\n",
        "\n",
        "4. Can use a pre-quantized 4bit BnB safetensor file from [Unsloth's HF page](https://huggingface.co/unsloth) or a full 16bit one, but must do QLoRA.\n",
        "\n",
        "5. Can use `accelerate` but must be FSDP2 or related - you can investigate https://github.com/huggingface/accelerate/pull/3394, Torch Titan, other repos etc.\n",
        "\n",
        "6. Must be fully `transformers` compatible - so we must use `TrainingArguments` and `Trainer`, or `TRL` related classes.\n",
        "\n",
        "7. The loss must be equivalent to single GPU training.\n",
        "\n",
        "8. You must enable all features in FSDP2 - ie showcase offloading, checkpointing, mixed precision training etc.\n",
        "\n",
        "9. You can use `nf4` from `torch AO`, but best from `bitsandbytes`.\n",
        "\n",
        "10. Finally showcase everything working in a free Kaggle 2x Tesla T4 notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sjo8K1lHAyI0"
      },
      "outputs": [],
      "source": [
        "# HELPFUL functions to undo Unsloth patches:\n",
        "import sys\n",
        "\n",
        "def remove_patched_module(package_name):\n",
        "    modules_to_delete = [\n",
        "        name for name in sys.modules\n",
        "        if name == package_name or name.startswith(package_name + \".\")\n",
        "    ]\n",
        "    for name in modules_to_delete: del sys.modules[name]\n",
        "\n",
        "remove_patched_module(\"trl\")\n",
        "remove_patched_module(\"transformers\")\n",
        "remove_patched_module(\"peft\")\n",
        "remove_patched_module(\"bitsandbytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QYoe7A0PwXY"
      },
      "source": [
        "Below is an example script which should run fine in Kaggle 2x Telsa T4s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "fxY0ycaRB4PY",
        "outputId": "c110d4c5-5b1b-4db6-b290-eed8effa97c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Commenting the code as it doesn\\'t work and throws an Exception\\nimport os\\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,\"    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\\n\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nfrom peft import get_peft_model, LoraConfig, TaskType\\nfrom transformers.models.llama.modeling_llama import LlamaDecoderLayer\\nfrom torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP, CPUOffload, MixedPrecision, BackwardPrefetch\\nfrom trl import SFTTrainer, SFTConfig\\nfrom datasets import load_dataset\\nfrom peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig, TaskType\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nimport os\\nimport sys\\nimport torch\\nimport torch.distributed as dist\\n\\nos.environ[\"RANK\"] = os.environ.get(\"RANK\", \"0\")\\nos.environ[\"WORLD_SIZE\"] = os.environ.get(\"WORLD_SIZE\", \"2\")\\nos.environ[\"MASTER_ADDR\"] = os.environ.get(\"MASTER_ADDR\", \"localhost\")\\nos.environ[\"MASTER_PORT\"] = os.environ.get(\"MASTER_PORT\", \"29500\")\\nlocal_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\\ndevice = torch.device(f\"cuda:{local_rank}\")\\n\\nmax_seq_length = 2048\\ntorch.set_default_dtype(torch.float16)\\nmodel_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\\ndtype = torch.float16\\ndef setup_distributed():\\n    if not dist.is_initialized():\\n        dist.init_process_group(backend=\"nccl\")\\n        print(f\"Distributed process group initialized on device {device}.\")\\n\\n\\nsetup_distributed()\\n\\n\\ndef auto_wrap_policy(module, recurse, nonwrapped_numel): return transformer_auto_wrap_policy(\\n    module, recurse, nonwrapped_numel, transformer_layer_cls=(\\n        LlamaDecoderLayer,)\\n)\\n\\n\\nfsdp_mixed_precision = MixedPrecision(\\n    param_dtype=torch.float16,\\n    reduce_dtype=torch.float16,\\n    buffer_dtype=torch.float16,\\n)\\nfsdp_config = {\\n    \"auto_wrap_policy\": auto_wrap_policy,\\n    \"mixed_precision\": fsdp_mixed_precision,\\n    \"cpu_offload\": CPUOffload(offload_params=True),\\n    \"backward_prefetch\": BackwardPrefetch.BACKWARD_PRE,\\n}\\n\\nmax_seq_length = 2048\\ntorch.set_default_dtype(torch.float16)\\nmodel_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\\ndtype = torch.float16\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_use_double_quant=True,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_compute_dtype=dtype,\\n)\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map={\"\": device},\\n    attn_implementation=\"sdpa\",\\n    quantization_config=bnb_config,\\n)\\n\\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\ntokenizer.padding_side = \"right\"\\n\\nlora_config = LoraConfig(\\n    r=64,\\n    lora_alpha=128,\\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\\n    lora_dropout=0,\\n    bias=\"none\",\\n    task_type=TaskType.CAUSAL_LM,\\n)\\nmodel = get_peft_model(model, lora_config)\\nwith torch.no_grad():\\n    for name, param in model.named_parameters():\\n        if \".lora_A.\" in name or \".lora_B.\" in name:\\n            param.requires_grad_(True)\\n        else:\\n            param.requires_grad_(False)\\n\\nmodel.gradient_checkpointing_enable()\\nmodel.enable_input_require_grads()\\n\\ndef convert_non_float_params_to_buffers(module):\\n    for name, param in list(module._parameters.items()):\\n        if param is not None and param.dtype not in (torch.float16, torch.float32, torch.float64, torch.bfloat16):\\n            module._parameters.pop(name)\\n            if hasattr(module, name):\\n                delattr(module, name)\\n            module.register_buffer(name, param)\\n    for child in module.children():\\n        convert_non_float_params_to_buffers(child)\\n\\n\\nconvert_non_float_params_to_buffers(model)\\n\\ndef cast_float32_to_float16(module):\\n    for name, param in module.named_parameters():\\n        if param is not None and param.dtype == torch.float32:\\n            param.data = param.data.half()\\n    for name, buf in module._buffers.items():\\n        if buf is not None and buf.dtype == torch.float32:\\n            module._buffers[name] = buf.half()\\n    for child in module.children():\\n        cast_float32_to_float16(child)\\n\\n\\ncast_float32_to_float16(model)\\n\\nmodel = FSDP(model, use_orig_params=True, **fsdp_config)\\n\\nurl = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\\ndataset = load_dataset(\"json\", data_files={\"train\": url}, split=\"train[:10%]\")\\n\\ndef custom_data_collator(features):\\n    input_ids = [f[\"text\"] for f in features]\\n\\n    batch = tokenizer(\\n        input_ids,\\n        padding=True,\\n        truncation=True,\\n        max_length=max_seq_length,\\n        return_tensors=\"pt\",\\n    )\\n\\n    batch[\"labels\"] = batch[\"input_ids\"].clone()\\n\\n    return batch\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Commenting the code as it doesn't work and throws an Exception\n",
        "import os\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,\"\\\n",
        "    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
        "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, CPUOffload, MixedPrecision, BackwardPrefetch\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datasets import load_dataset\n",
        "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig, TaskType\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "os.environ[\"RANK\"] = os.environ.get(\"RANK\", \"0\")\n",
        "os.environ[\"WORLD_SIZE\"] = os.environ.get(\"WORLD_SIZE\", \"2\")\n",
        "os.environ[\"MASTER_ADDR\"] = os.environ.get(\"MASTER_ADDR\", \"localhost\")\n",
        "os.environ[\"MASTER_PORT\"] = os.environ.get(\"MASTER_PORT\", \"29500\")\n",
        "local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
        "device = torch.device(f\"cuda:{local_rank}\")\n",
        "\n",
        "max_seq_length = 2048\n",
        "torch.set_default_dtype(torch.float16)\n",
        "model_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "dtype = torch.float16\n",
        "def setup_distributed():\n",
        "    if not dist.is_initialized():\n",
        "        dist.init_process_group(backend=\"nccl\")\n",
        "        print(f\"Distributed process group initialized on device {device}.\")\n",
        "\n",
        "\n",
        "setup_distributed()\n",
        "\n",
        "\n",
        "def auto_wrap_policy(module, recurse, nonwrapped_numel): return transformer_auto_wrap_policy(\n",
        "    module, recurse, nonwrapped_numel, transformer_layer_cls=(\n",
        "        LlamaDecoderLayer,)\n",
        ")\n",
        "\n",
        "\n",
        "fsdp_mixed_precision = MixedPrecision(\n",
        "    param_dtype=torch.float16,\n",
        "    reduce_dtype=torch.float16,\n",
        "    buffer_dtype=torch.float16,\n",
        ")\n",
        "fsdp_config = {\n",
        "    \"auto_wrap_policy\": auto_wrap_policy,\n",
        "    \"mixed_precision\": fsdp_mixed_precision,\n",
        "    \"cpu_offload\": CPUOffload(offload_params=True),\n",
        "    \"backward_prefetch\": BackwardPrefetch.BACKWARD_PRE,\n",
        "}\n",
        "\n",
        "max_seq_length = 2048\n",
        "torch.set_default_dtype(torch.float16)\n",
        "model_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "dtype = torch.float16\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=dtype,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map={\"\": device},\n",
        "    attn_implementation=\"sdpa\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=128,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "        if \".lora_A.\" in name or \".lora_B.\" in name:\n",
        "            param.requires_grad_(True)\n",
        "        else:\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "def convert_non_float_params_to_buffers(module):\n",
        "    for name, param in list(module._parameters.items()):\n",
        "        if param is not None and param.dtype not in (torch.float16, torch.float32, torch.float64, torch.bfloat16):\n",
        "            module._parameters.pop(name)\n",
        "            if hasattr(module, name):\n",
        "                delattr(module, name)\n",
        "            module.register_buffer(name, param)\n",
        "    for child in module.children():\n",
        "        convert_non_float_params_to_buffers(child)\n",
        "\n",
        "\n",
        "convert_non_float_params_to_buffers(model)\n",
        "\n",
        "def cast_float32_to_float16(module):\n",
        "    for name, param in module.named_parameters():\n",
        "        if param is not None and param.dtype == torch.float32:\n",
        "            param.data = param.data.half()\n",
        "    for name, buf in module._buffers.items():\n",
        "        if buf is not None and buf.dtype == torch.float32:\n",
        "            module._buffers[name] = buf.half()\n",
        "    for child in module.children():\n",
        "        cast_float32_to_float16(child)\n",
        "\n",
        "\n",
        "cast_float32_to_float16(model)\n",
        "\n",
        "model = FSDP(model, use_orig_params=True, **fsdp_config)\n",
        "\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": url}, split=\"train[:10%]\")\n",
        "\n",
        "def custom_data_collator(features):\n",
        "    input_ids = [f[\"text\"] for f in features]\n",
        "\n",
        "    batch = tokenizer(\n",
        "        input_ids,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
        "\n",
        "    return batch\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71dEjjtGmtuH"
      },
      "source": [
        "Reminder your code must have the same loss curve over 60 steps or so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "BV2WK_wEDtUn",
        "outputId": "cfd3f566-c957-49e5-da98-372d3809935e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'trainer = SFTTrainer(\\n    model=model,\\n    train_dataset=dataset,\\n    data_collator=custom_data_collator,\\n    args=SFTConfig(\\n        per_device_train_batch_size=2,\\n        gradient_accumulation_steps=4,\\n        warmup_steps=1,\\n        max_steps=10,\\n        logging_steps=1,\\n        output_dir=\"outputs\",\\n        seed=3407,\\n        max_seq_length=max_seq_length,\\n        fp16=True,\\n        bf16=False,\\n        report_to=\"none\",\\n        dataset_num_proc=4,\\n        remove_unused_columns=False,\\n        dataloader_num_workers=0,\\n    ),\\n)\\n\\n\\ntrainer.train()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\"\"\"trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=custom_data_collator,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=1,\n",
        "        max_steps=10,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        seed=3407,\n",
        "        max_seq_length=max_seq_length,\n",
        "        fp16=True,\n",
        "        bf16=False,\n",
        "        report_to=\"none\",\n",
        "        dataset_num_proc=4,\n",
        "        remove_unused_columns=False,\n",
        "        dataloader_num_workers=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Mnm9AlVvpPhb"
      },
      "outputs": [],
      "source": [
        "# del model\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "I tried to make it work by changing parameters on the high level, but did not dive deep into implementing it because honestly, I was more interested in the Triton kernel (Challenge A)."
      ],
      "metadata": {
        "id": "LTejGO4tR-hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring():\n",
        "    global final_score\n",
        "    print(\"‚ùå Unsuccessful Attempt: B_score = 0\")\n",
        "    print(f\"Final Score: {final_score}\")\n",
        "scoring()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d7kzPOBSAU9",
        "outputId": "a70819b1-5e62-4865-c52e-dddf784f63c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Unsuccessful Attempt: B_score = 0\n",
            "Final Score: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vIg_ZjYt1Tq"
      },
      "source": [
        "## Marking Criteria for B) Max points = 10\n",
        "```python\n",
        "if attemped_B:\n",
        "    B_score = 0\n",
        "    if FSDP2_works_with_QLoRA:\n",
        "        if torch_compile_works: B_score += 5\n",
        "        else: B_score += 3\n",
        "        if uses_part_A_and_single_kernel_and_faster: B_score += 3\n",
        "        elif uses_torchAO:\n",
        "            if torchAO_slower_than_BnB: B_score -= 3\n",
        "    elif TP_or_PP_with_QLoRA:\n",
        "        if zero_bubble: B_score += 3\n",
        "        else: B_score += 2\n",
        "    elif FSDP1_works_with_QLoRA:\n",
        "        B_score += 1\n",
        "    if kaggle_notebook_2_tesla_t4_example:\n",
        "        B_score += 2\n",
        "    else:\n",
        "        B_score = 0\n",
        "    final_score += B_score\n",
        "else:\n",
        "    final_score -= 2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pukEsR2YnIHQ"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"COMPILE\"></a>\n",
        "## C) Make `torch.compile` work without graph breaks for QLoRA [Difficulty: Easy to Medium] [Max points: 9]\n",
        "\n",
        "1. Goal: Write a single Python script like task B), except the goal is to `torch.compile` all modules if possible.\n",
        "\n",
        "2. There must NOT be graph breaks, and excessive re-compilations should not be seen.\n",
        "\n",
        "3. You should have say max 30 compilations. Over 60 is definitely wrong.\n",
        "\n",
        "4. The loss must match with the non compiled module.\n",
        "\n",
        "5. Utilize patching as much as possible.\n",
        "\n",
        "6. Think about which areas might need disabling for compilation. Think about regional compilation. How do we compile sections efficiently?\n",
        "\n",
        "7. Log memory / VRAM usage, and monitor speedups as well.\n",
        "\n",
        "8. Must work for QLoRA.\n",
        "\n",
        "We provided a script below, and showcased how to detect if graph breaks are seen. We also torch compiled the MLP for Llama:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Again, commenting out Challenge C as I did not attempt it."
      ],
      "metadata": {
        "id": "YFEOgiB_XrQG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QFOXncAVNqmK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "0a5fb4ae-aec1-4f4b-dcbc-1430b8eb43ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import torch\\ntorch_compile_options = torch_compile_options = {\\n    \"epilogue_fusion\"   : True,\\n    \"max_autotune\"      : True,\\n    \"shape_padding\"     : True,\\n    \"trace.enabled\"     : True,\\n    \"triton.cudagraphs\" : False,\\n}\\n\\n@torch.compile(fullgraph = False, dynamic = True, options = torch_compile_options)\\ndef compiled_llama_mlp(self, x):\\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\\n    return down_proj\\n\\nimport transformers.models.llama.modeling_llama\\ntransformers.models.llama.modeling_llama.LlamaMLP.forward = compiled_llama_mlp'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\"\"\"import torch\n",
        "torch_compile_options = torch_compile_options = {\n",
        "    \"epilogue_fusion\"   : True,\n",
        "    \"max_autotune\"      : True,\n",
        "    \"shape_padding\"     : True,\n",
        "    \"trace.enabled\"     : True,\n",
        "    \"triton.cudagraphs\" : False,\n",
        "}\n",
        "\n",
        "@torch.compile(fullgraph = False, dynamic = True, options = torch_compile_options)\n",
        "def compiled_llama_mlp(self, x):\n",
        "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "    return down_proj\n",
        "\n",
        "import transformers.models.llama.modeling_llama\n",
        "transformers.models.llama.modeling_llama.LlamaMLP.forward = compiled_llama_mlp\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "WmoQzMDzm1zL",
        "outputId": "252dc0db-0201-40c1-d58d-05c0c5981b78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nfrom peft import get_peft_model, LoraConfig, TaskType\\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] =     \"expandable_segments:True,\"    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\\n\\nmax_seq_length = 1024\\ntorch.set_default_dtype(torch.float16)\\nmodel_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\\ndtype = torch.float16\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit              = True,\\n    bnb_4bit_use_double_quant = True,\\n    bnb_4bit_quant_type       = \"nf4\",\\n    bnb_4bit_compute_dtype    = dtype,\\n)\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map = \"auto\",\\n    attn_implementation = \"sdpa\",\\n    quantization_config = bnb_config,\\n)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\ntokenizer.padding_side = \"right\"\\n\\nlora_config = LoraConfig(\\n    r = 32,\\n    lora_alpha = 64,\\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\\n    lora_dropout = 0,\\n    bias = \"none\",\\n    task_type = TaskType.CAUSAL_LM,\\n)\\n\\n# Get LoRA and setup model\\nmodel = get_peft_model(model, lora_config)\\nwith torch.no_grad():\\n    for name, param in model.named_parameters():\\n        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\\n        else: param.requires_grad_(False)\\n\\n# Currently GC will cause torch.compile to be disabled, so disable it\\n# model.gradient_checkpointing_enable()\\nmodel.enable_input_require_grads()\\n\\n# Get dataset\\nfrom datasets import load_dataset\\nfrom trl import SFTTrainer, SFTConfig\\nurl = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\\ndataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\"\"\"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \\\n",
        "    \"expandable_segments:True,\"\\\n",
        "    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
        "\n",
        "max_seq_length = 1024\n",
        "torch.set_default_dtype(torch.float16)\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "dtype = torch.float16\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit              = True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type       = \"nf4\",\n",
        "    bnb_4bit_compute_dtype    = dtype,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map = \"auto\",\n",
        "    attn_implementation = \"sdpa\",\n",
        "    quantization_config = bnb_config,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r = 32,\n",
        "    lora_alpha = 64,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    task_type = TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Get LoRA and setup model\n",
        "model = get_peft_model(model, lora_config)\n",
        "with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n",
        "        else: param.requires_grad_(False)\n",
        "\n",
        "# Currently GC will cause torch.compile to be disabled, so disable it\n",
        "# model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# Get dataset\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbVNGNQ5LlpJ"
      },
      "source": [
        "We provide full logging for `torch.compile` like below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EsekFGdsK5hZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "eab5f5b7-1782-41b2-aaef-b79d5b9075b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Must show all graph breaks are not seen with torch.compile\\nimport os\\nos.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\\nos.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\\nos.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\\n\\nimport logging\\ntorch._inductor.config.debug = True\\ntorch._logging.set_logs(\\n    dynamo = logging.WARN,\\n    inductor = logging.WARN,\\n    graph_breaks = True,\\n    recompiles = True,\\n    recompiles_verbose = True,\\n    compiled_autograd_verbose = True,\\n    # aot_joint_graph = True, # Enable for more logs\\n    # aot_graphs = True,\\n)\\ntorch._dynamo.config.verbose = True\\ntorch._dynamo.config.suppress_errors = False'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "\"\"\"# Must show all graph breaks are not seen with torch.compile\n",
        "import os\n",
        "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
        "\n",
        "import logging\n",
        "torch._inductor.config.debug = True\n",
        "torch._logging.set_logs(\n",
        "    dynamo = logging.WARN,\n",
        "    inductor = logging.WARN,\n",
        "    graph_breaks = True,\n",
        "    recompiles = True,\n",
        "    recompiles_verbose = True,\n",
        "    compiled_autograd_verbose = True,\n",
        "    # aot_joint_graph = True, # Enable for more logs\n",
        "    # aot_graphs = True,\n",
        ")\n",
        "torch._dynamo.config.verbose = True\n",
        "torch._dynamo.config.suppress_errors = False\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RdragY7P-dL"
      },
      "source": [
        "When we execute the code below, we can see graph breaks - remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "wHOBZGLepYgg",
        "outputId": "49966248-113e-42a1-b0d0-133fd3cc191e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'trainer = SFTTrainer(\\n    model = model,\\n    train_dataset = dataset,\\n    processing_class = tokenizer,\\n    args = SFTConfig(\\n        per_device_train_batch_size = 1,\\n        gradient_accumulation_steps = 2,\\n        warmup_steps = 1,\\n        max_steps = 10,\\n        logging_steps = 1,\\n        output_dir = \"outputs\",\\n        seed = 3407,\\n        max_seq_length = max_seq_length,\\n        fp16 = model.get_input_embeddings().weight.dtype == torch.float16,\\n        bf16 = model.get_input_embeddings().weight.dtype == torch.bfloat16,\\n        report_to = \"none\", # For W&B\\n        dataset_num_proc = 4,\\n    ),\\n)\\ntrainer.train()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "\"\"\"trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    processing_class = tokenizer,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 2,\n",
        "        warmup_steps = 1,\n",
        "        max_steps = 10,\n",
        "        logging_steps = 1,\n",
        "        output_dir = \"outputs\",\n",
        "        seed = 3407,\n",
        "        max_seq_length = max_seq_length,\n",
        "        fp16 = model.get_input_embeddings().weight.dtype == torch.float16,\n",
        "        bf16 = model.get_input_embeddings().weight.dtype == torch.bfloat16,\n",
        "        report_to = \"none\", # For W&B\n",
        "        dataset_num_proc = 4,\n",
        "    ),\n",
        ")\n",
        "trainer.train()\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPbbwd5uDOL1"
      },
      "source": [
        "Log all your steps for debugging in a Colab (maybe this one). Edward's blog http://blog.ezyang.com/, Horace's blogs https://www.thonking.ai/, Slaying OOMs by Jane & Mark: ttps://www.youtube.com/watch?v=UvRl4ansfCg could be useful."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "I made an attempt with the help of LLMs, but it didn't work."
      ],
      "metadata": {
        "id": "9utDG1qvTi8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring():\n",
        "    global final_score\n",
        "    print(\"‚ùå No Attempt: final_score -= 1\")\n",
        "    final_score -= 1\n",
        "    print(f\"Final Score: {final_score}\")\n",
        "scoring()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_2U3ew0TiHv",
        "outputId": "24e9a639-158c-4ff7-9059-4d86bbb1f19b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå No Attempt: final_score -= 1\n",
            "Final Score: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wojg8SDjv3fu"
      },
      "source": [
        "## Marking Criteria for C) Max points = 9\n",
        "```python\n",
        "if attemped_C:\n",
        "    C_score = 0\n",
        "    if uses_flex_attention:\n",
        "        if dynamic_sequence_length_works: C_score += 3\n",
        "        else: C_score += 1\n",
        "    if no_torch_compile_BnB: C_score -= 2\n",
        "    elif use_part_A: C_score += 1\n",
        "    elif torch_compile_BnB: C_score += 1\n",
        "\n",
        "    if attention_compiled:\n",
        "        if excessive_recompilation: C_score -= 3\n",
        "        else: C_score += 2\n",
        "    if mlp_compiled:\n",
        "        if excessive_recompilation: C_score -= 3\n",
        "        C_score += 1\n",
        "\n",
        "    if not loss_compiled: C_score -= 1\n",
        "    if not layernorms_compiled: C_score -= 3\n",
        "\n",
        "    if max_autotune_triton_matmul:\n",
        "        if excessive_recompilation: C_score -= 2\n",
        "        else: C_score += 2\n",
        "    \n",
        "    final_score += C_score\n",
        "else:\n",
        "    final_score -= 1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKcvFLCsQLtL"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"ISSUES\"></a>\n",
        "## D) Help solve ü¶• Unsloth issues! [Difficulty: Varies] [Max points: 12]\n",
        "\n",
        "Head over to https://github.com/unslothai/unsloth, and find some issues which are still left standing / not resolved. The tag **currently fixing** might be useful.\n",
        "\n",
        "Each successfully accepted and solved issue will also have \\$100 to \\$1000 of bounties.\n",
        "\n",
        "It's best to attempt these features:\n",
        "\n",
        "* **<ins>Tool Calling</ins>** [Points = 1] Provide a tool calling Colab notebook and make it work inside of Unsloth. <ins>Bounty: \\$1000</ins>\n",
        "\n",
        "* **<ins>GGUF Vision support</ins>** [Points = 1] Allow exporting vision finetunes to GGUF directly. Llava and Qwen VL must work. <ins>Bounty: \\$500</ins>\n",
        "\n",
        "* **<ins>Refactor Attention</ins>** [Points = 2] Refactor and merge xformers, SDPA, flash-attn, flex-attention into a simpler interface. Must work seamlessly inside of Unsloth. <ins>Bounty: \\$350</ins>\n",
        "\n",
        "* **<ins>Windows support</ins>** [Points = 2] Allow `pip install unsloth` to work in Windows - Triton, Xformers, bitsandbytes should all function. You might need to edit `pyproject.toml`. Confirm it works. <ins>Bounty: \\$300</ins>\n",
        "\n",
        "* **<ins>Support Sequence Classification</ins>** [Points = 1] Create patching functions to patch over AutoModelForSequenceClassification, and allow finetuner to use AutoModelForSequenceClassification. <ins>Bounty: \\$200</ins>\n",
        "\n",
        "* **<ins>VLMs Data Collator</ins>** [Points = 1] Make text & image mixing work efficiently -so some inputs can be text only. Must work on Qwen, Llama, Pixtral. <ins>Bounty: \\$100</ins>\n",
        "\n",
        "* **<ins>VLMs image resizing</ins>** [Points = 1] Allow finetuner to specify maximum image size, or get it from the config.json file. Resize all images to specific size to reduce VRAM. <ins>Bounty: \\$100</ins>\n",
        "\n",
        "* **<ins>Support Flex Attention</ins>** [Points = 2] Allow dynamic sequence lengths without excessive recompilation. Make this work on SWAs and normal causal masks. Also packed sequence masks. <ins>Bounty: \\$100</ins>\n",
        "\n",
        "* **<ins>VLMs train only on completions</ins>** [Points = 1] Edit `train_on_responses_only` to allow it to work on VLMs. <ins>Bounty: \\$100</ins>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "I did not attempt this."
      ],
      "metadata": {
        "id": "jhlgvoxgUArz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring():\n",
        "    print(\"‚ùå No Attempt.\")\n",
        "    print(f\"Final Score: {final_score}\")\n",
        "scoring()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2GL3lxdUEmT",
        "outputId": "1be7d947-610d-477e-9905-4a7ea88b310c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå No Attempt.\n",
            "Final Score: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VfYZjuMxujG"
      },
      "source": [
        "## Marking Criteria for D) Max points = 12\n",
        "```python\n",
        "if attemped_D:\n",
        "    D_score = 0\n",
        "    for subtask in subtasks:\n",
        "        if sucessfully_completed_subtask:\n",
        "            D_score += score_for_subtask\n",
        "    final_score += D_score\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrJzggfH2YEG"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"MATH\"></a>\n",
        "## E) Memory Efficient Backprop [Difficulty: Medium to Hard] [Max points: 10]\n",
        "\n",
        "In LLMs, the last layer is a projection matrix to calculate the probabilities of the next token, ie $\\sigma(XW)$. However, if the vocabulary size is very large, say 128K, then the materialization of the logits causes VRAM spikes.\n",
        "\n",
        "For example, if the `bsz = 4, qlen = 4096, hd = 4096, vocab = 128K`, then the memory usage for the logits in bfloat16 would be 4GB. In the worst case, we might even need to upcast logits to float32, so 8GB is needed.\n",
        "\n",
        "In Unsloth, we utilize [Apple's Cut Cross Entropy Loss](https://machinelearning.apple.com/research/cut-your-losses) to reduce VRAM usage, by allowing a Triton kernel to create the logits on the fly to calculate the cross entropy loss. But this does not generalize well to other functions.\n",
        "\n",
        "Our goal is to generalize this ultimately, but directly creating logits on the fly will be hard. Instead, let's take a slightly less complex approach. Let's first review some stuff. We first notice that during the normal case after forming the intermediate logits for 2 batches, we then do a gather function to aggregate the intermediate results into a single column:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\times W &= \\begin{bmatrix} x_1 W \\\\ x_2 W \\end{bmatrix} \\\\\n",
        "f \\bigg( \\begin{bmatrix} x_1 W \\\\ x_2 W \\end{bmatrix} \\bigg) &= \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "So, if we can somehow skip the materialization of the intermediate logits, and just output the output of `f`, we can save a lot of VRAM!\n",
        "\n",
        "Notice during backpropagation we can use the chain rule:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{dX} &= \\frac{dL}{dy} \\frac{dy}{dX} ; \\frac{dL}{dW} = \\frac{dL}{dy} \\frac{dy}{dW} \\\\\n",
        "\\frac{dL}{dy} &= \\text{Downstream from backprop} \\\\\n",
        "\\frac{dy}{dX} &= W^T \\\\\n",
        "\\frac{dy}{dW} &= X^T \\\\\n",
        "\\frac{dL}{dX} &= \\frac{dL}{dy} W^T \\\\\n",
        "\\frac{dL}{dW} &= X^T \\frac{dL}{dy} \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "If we simply compute the intermediate tensors on the fly via batches, say we do batch 1, then batch 2, we can reduce VRAM usage from 4GB to 2GB!\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{dX} &= \\begin{bmatrix} \\frac{dL_1}{dy_1} W^T \\\\ \\frac{dL_2}{dy_2} W^T \\end{bmatrix} \\\\\n",
        "\\frac{dL}{dW} &= \\bigg( X_1^T \\frac{dL_1}{dy_1} + X_2^T  \\frac{dL_2}{dy_2} \\bigg)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "1. Your goal is to write a `torch.autograd.Function` with a `forward` and `backward` pass showcasing this memory efficient implementation.\n",
        "\n",
        "2. You must NOT hard code the derivatives - move the transformation function from the logits / intermeditate tensors to a smaller tensor as a separate function which can allow `autograd` to pass through it.\n",
        "\n",
        "3. As a hint, look at `torch.checkpoint` at https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py. Also, don't forget about the upstream gradients! We need to multiply them to the current gradients!\n",
        "\n",
        "4. Make the Cross Entropy Loss work. You must show other functions working as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Rp-IJIbv90f6"
      },
      "outputs": [],
      "source": [
        "def transformation_function(batch, linear, labels):\n",
        "    x = linear(batch).float() # Up projection to large space\n",
        "    from torch.nn import CrossEntropyLoss\n",
        "    down_projection_function = CrossEntropyLoss(reduction = \"mean\")\n",
        "    # Down projection to small space\n",
        "    loss = down_projection_function(x.view(-1, x.shape[-1]), labels.view(-1))\n",
        "    return loss\n",
        "\n",
        "def transformation_function_kld_loss(batch, linear, labels):\n",
        "    x = linear(batch).float() # Up projection to large space\n",
        "    from torch.nn import KLDivLoss\n",
        "    down_projection_function = KLDivLoss(reduction='batchmean')\n",
        "    # Down projection to small space\n",
        "    x_view = x.view(-1, x.shape[-1])\n",
        "    loss = down_projection_function(x_view, torch.nn.functional.one_hot(labels.view(-1), num_classes=x_view.shape[-1]).float())\n",
        "    return loss\n",
        "\n",
        "def transformation_function_mse_loss(batch, linear, labels):\n",
        "    x = linear(batch).float() # Up projection to large space\n",
        "    from torch.nn import MSELoss\n",
        "    down_projection_function = MSELoss()\n",
        "    # Down projection to small space\n",
        "    x_view = x.view(-1, x.shape[-1])\n",
        "    loss = down_projection_function(x_view, torch.nn.functional.one_hot(labels.view(-1), num_classes=x_view.shape[-1]).float())\n",
        "    return loss\n",
        "\n",
        "def standard_forward(X, linear, labels, transformation_fn):\n",
        "    return transformation_fn(X, linear, labels)\n",
        "\n",
        "\n",
        "\n",
        "class MemoryEfficientLinear(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, X, linear, labels, forward_function, chunk_size):\n",
        "\n",
        "        ctx.linear = linear\n",
        "        ctx.labels = labels\n",
        "        ctx.forward_function = forward_function\n",
        "        ctx.save_for_backward(X)\n",
        "        ctx.chunk_size = chunk_size\n",
        "\n",
        "        B, L = X.shape[0], X.shape[1]\n",
        "        flat_X = X.view(-1, X.shape[-1])\n",
        "        flat_labels = labels.view(-1)\n",
        "        total_tokens = flat_X.shape[0]\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for i in range(0, total_tokens, chunk_size):\n",
        "            X_chunk = flat_X[i: i + chunk_size].detach()\n",
        "            labels_chunk = flat_labels[i: i + chunk_size]\n",
        "            with torch.no_grad():\n",
        "                loss_chunk = forward_function(X_chunk, linear, labels_chunk).to(X.device)\n",
        "            total_loss += loss_chunk * X_chunk.shape[0]\n",
        "\n",
        "        final_loss = total_loss / total_tokens\n",
        "        return final_loss\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, d_loss):\n",
        "        X, = ctx.saved_tensors\n",
        "        labels = ctx.labels\n",
        "        linear = ctx.linear\n",
        "        forward_function = ctx.forward_function\n",
        "        chunk_size = ctx.chunk_size\n",
        "\n",
        "        flat_X = X.view(-1, X.shape[-1])\n",
        "        flat_labels = labels.view(-1)\n",
        "        total_tokens = flat_X.shape[0]\n",
        "\n",
        "        grad_flat_X = torch.zeros_like(flat_X)\n",
        "        grad_weight = torch.zeros_like(linear.weight)\n",
        "        grad_bias = torch.zeros_like(linear.bias)\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            for i in range(0, total_tokens, chunk_size):\n",
        "                X_chunk = flat_X[i: i + chunk_size].detach().requires_grad_()\n",
        "                labels_chunk = flat_labels[i: i + chunk_size]\n",
        "                loss_chunk = forward_function(X_chunk, linear, labels_chunk)\n",
        "\n",
        "                grads = torch.autograd.grad(\n",
        "                    loss_chunk, (X_chunk, linear.weight, linear.bias),\n",
        "                    retain_graph=True, allow_unused=True\n",
        "                )\n",
        "                grad_X_chunk, grad_weight_chunk, grad_bias_chunk = grads\n",
        "                grad_flat_X[i: i + chunk_size] = grad_X_chunk * d_loss\n",
        "                if grad_weight_chunk is not None:\n",
        "                    grad_weight += grad_weight_chunk * d_loss\n",
        "                if grad_bias_chunk is not None:\n",
        "                    grad_bias += grad_bias_chunk * d_loss\n",
        "\n",
        "        grad_X = grad_flat_X.view_as(X)\n",
        "\n",
        "        if linear.weight.grad is None:\n",
        "            linear.weight.grad = grad_weight\n",
        "        else:\n",
        "            linear.weight.grad += grad_weight\n",
        "        if linear.bias.grad is None:\n",
        "            linear.bias.grad = grad_bias\n",
        "        else:\n",
        "            linear.bias.grad += grad_bias\n",
        "\n",
        "        return grad_X, None, None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def testing(bsz, qlen, hd, vocab, input_size, chunk_size, transformation_fn=transformation_function):\n",
        "    torch.manual_seed(42)\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    standard_implementation_OOM = False\n",
        "    efficient_implementation_OOM = False\n",
        "\n",
        "    X_standard = torch.randn(bsz, qlen, hd, requires_grad=True, device=device)\n",
        "    X_memory   = X_standard.clone().detach().requires_grad_()\n",
        "    labels = torch.randint(0, vocab, (bsz, qlen), device=device)\n",
        "\n",
        "    # Two identical linear layers.\n",
        "    linear_standard = nn.Linear(hd, vocab, device=device)\n",
        "    linear_memory   = nn.Linear(hd, vocab, device=device)\n",
        "    linear_memory.load_state_dict(linear_standard.state_dict())\n",
        "\n",
        "    # Standard implementation.\n",
        "    try:\n",
        "        loss_standard = standard_forward(X_standard, linear_standard, labels, transformation_fn)\n",
        "        loss_standard.backward()\n",
        "\n",
        "    except torch.OutOfMemoryError:\n",
        "        standard_implementation_OOM = True\n",
        "\n",
        "        X_standard.grad = None\n",
        "        linear_standard.weight.grad = None\n",
        "        linear_standard.bias.grad = None\n",
        "        try:\n",
        "            loss_standard.grad = None\n",
        "            del(loss_standard)\n",
        "        except:\n",
        "            pass\n",
        "        del(X_standard)\n",
        "        del(linear_standard)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    # Reset gradients before memory‚Äìefficient run.\n",
        "    X_memory.grad = None\n",
        "    linear_memory.weight.grad = None\n",
        "    linear_memory.bias.grad = None\n",
        "\n",
        "    # Memory‚Äìefficient implementation.\n",
        "    try:\n",
        "        loss_memory = MemoryEfficientLinear.apply(X_memory, linear_memory, labels, transformation_fn, chunk_size)\n",
        "        loss_memory.backward()\n",
        "\n",
        "    except torch.OutOfMemoryError:\n",
        "        efficient_implementation_OOM = True\n",
        "\n",
        "        X_memory.grad = None\n",
        "        linear_memory.weight.grad = None\n",
        "        linear_memory.bias.grad = None\n",
        "        try:\n",
        "            loss_memory.grad = None\n",
        "            del(loss_memory)\n",
        "        except:\n",
        "            pass\n",
        "        del(X_memory)\n",
        "        del(linear_memory)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\n----------- Run Summary for input size: {input_size} ----------------------\")\n",
        "    if not standard_implementation_OOM:\n",
        "        print(f\"‚úÖ Standard implementation ran successfully for input size: {input_size}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Standard implementation OutOfMemoryError for input_size: {input_size}\")\n",
        "    if not efficient_implementation_OOM:\n",
        "        print(f\"‚úÖ Efficient implementation ran successfully for input size: {input_size}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Efficient implementation OutOfMemoryError for input_size: {input_size}\")\n",
        "\n",
        "\n",
        "    if (standard_implementation_OOM == False) and (efficient_implementation_OOM == False):\n",
        "        grad_X_standard = X_standard.grad.clone()\n",
        "        grad_X_memory = X_memory.grad.clone()\n",
        "\n",
        "        print(f\"\\n------- Comparing Loss and Gradients for input size: {input_size} ------------\")\n",
        "\n",
        "        print(\"Standard Loss:                      \", loss_standard.item())\n",
        "        print(\"Memory Efficient Loss:              \", loss_memory.item())\n",
        "        print(\"Gradients equal (torch.allclose):   \", torch.allclose(grad_X_standard, grad_X_memory, atol=1e-4))\n",
        "        print(\"Max grad difference:                \", (grad_X_standard - grad_X_memory).abs().max().item())\n",
        "\n",
        "        X_standard.grad = None\n",
        "        linear_standard.weight.grad = None\n",
        "        linear_standard.bias.grad = None\n",
        "        loss_standard.grad = None\n",
        "        del(loss_standard)\n",
        "        del(X_standard)\n",
        "        del(linear_standard)\n",
        "        torch.cuda.empty_cache()\n",
        "        X_memory.grad = None\n",
        "        linear_memory.weight.grad = None\n",
        "        linear_memory.bias.grad = None\n",
        "        loss_memory.grad = None\n",
        "        del(loss_memory)\n",
        "        del(X_memory)\n",
        "        del(linear_memory)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    bsz, qlen, hd, vocab = 8, 4096, 4096, 64 * (2**10)\n",
        "    testing(bsz, qlen, hd, vocab, input_size='large', chunk_size=bsz*qlen//4)\n",
        "\n",
        "    bsz, qlen, hd, vocab = 4, 1024, 4096, 64 * (2**10)\n",
        "    testing(bsz, qlen, hd, vocab, input_size='small', chunk_size=bsz*qlen//4)\n",
        "\n",
        "    print(f\"\\n------- Running with KLD Loss ------------\")\n",
        "\n",
        "    testing(bsz, qlen, hd, vocab, input_size='small', chunk_size=bsz*qlen//4, transformation_fn=transformation_function_kld_loss)\n",
        "\n",
        "    print(f\"\\n------- Running with MSE Loss ------------\")\n",
        "\n",
        "    testing(bsz, qlen, hd, vocab, input_size='small', chunk_size=bsz*qlen//4, transformation_fn=transformation_function_mse_loss)\n",
        "\n",
        "    print(f\"\\n------- Running with chunk size = bsz*qlen//2 (default=bsz*qlen//4) ------------\")\n",
        "\n",
        "    testing(bsz, qlen, hd, vocab, input_size='small', chunk_size=bsz*qlen//2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX0W6oAypeDz",
        "outputId": "9820e77e-1e1c-441a-cc12-130595389fee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------- Run Summary for input size: large ----------------------\n",
            "‚ùå Standard implementation OutOfMemoryError for input_size: large\n",
            "‚úÖ Efficient implementation ran successfully for input size: large\n",
            "\n",
            "----------- Run Summary for input size: small ----------------------\n",
            "‚úÖ Standard implementation ran successfully for input size: small\n",
            "‚úÖ Efficient implementation ran successfully for input size: small\n",
            "\n",
            "------- Comparing Loss and Gradients for input size: small ------------\n",
            "Standard Loss:                       11.24808120727539\n",
            "Memory Efficient Loss:               11.248079299926758\n",
            "Gradients equal (torch.allclose):    True\n",
            "Max grad difference:                 1.1692042789945845e-05\n",
            "\n",
            "------- Running with KLD Loss ------------\n",
            "\n",
            "----------- Run Summary for input size: small ----------------------\n",
            "‚úÖ Standard implementation ran successfully for input size: small\n",
            "‚úÖ Efficient implementation ran successfully for input size: small\n",
            "\n",
            "------- Comparing Loss and Gradients for input size: small ------------\n",
            "Standard Loss:                       -0.00895950198173523\n",
            "Memory Efficient Loss:               -0.00895950198173523\n",
            "Gradients equal (torch.allclose):    True\n",
            "Max grad difference:                 1.1444091796875e-05\n",
            "\n",
            "------- Running with MSE Loss ------------\n",
            "\n",
            "----------- Run Summary for input size: small ----------------------\n",
            "‚úÖ Standard implementation ran successfully for input size: small\n",
            "‚úÖ Efficient implementation ran successfully for input size: small\n",
            "\n",
            "------- Comparing Loss and Gradients for input size: small ------------\n",
            "Standard Loss:                       0.33334845304489136\n",
            "Memory Efficient Loss:               0.33334845304489136\n",
            "Gradients equal (torch.allclose):    True\n",
            "Max grad difference:                 6.700225299027807e-07\n",
            "\n",
            "------- Running with chunk size = bsz*qlen//2 (default=bsz*qlen//4) ------------\n",
            "\n",
            "----------- Run Summary for input size: small ----------------------\n",
            "‚úÖ Standard implementation ran successfully for input size: small\n",
            "‚úÖ Efficient implementation ran successfully for input size: small\n",
            "\n",
            "------- Comparing Loss and Gradients for input size: small ------------\n",
            "Standard Loss:                       11.24808120727539\n",
            "Memory Efficient Loss:               11.248082160949707\n",
            "Gradients equal (torch.allclose):    True\n",
            "Max grad difference:                 3.897387614415493e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lIczyn8-2-o"
      },
      "source": [
        "To test your implementation, it should not OOM for large inputs. Also, check the gradient is actually equivalent via `torch.allclose` in the normal approach."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Llama 3.2 1B Training Loss"
      ],
      "metadata": {
        "id": "6wup2dahNw73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following are the loss values for 60 epochs of training Llama 3.2 1B with the memory efficient backprop implementation.\n",
        "\n",
        "# Colab Link: https://colab.research.google.com/drive/1kQVHBcjPW6sRseSDuLDKDCTE1fTXQeUL?usp=sharing\n",
        "# Github Link: https://github.com/RohitNagraj/UnslothPuzzles/blob/main/resources/q5/Llama3_2_1B_memory_efficient.ipynb\n",
        "\n",
        "llama32_1b_training_loss_memory_efficient = [\n",
        "    0.877800, 0.965500, 1.159700, 1.032800, 0.826000, 1.079700, 0.750300,\n",
        "    1.134900, 1.015500, 0.884800, 1.010100, 1.255500, 1.073200, 0.745800,\n",
        "    1.009500, 0.740900, 1.143400, 0.983700, 0.885700, 1.014700, 1.005600,\n",
        "    0.987100, 1.146500, 0.995700, 0.758100, 0.942400, 0.965600, 0.902200,\n",
        "    1.254500, 1.179900, 0.818600, 0.631500, 0.752300, 0.700100, 0.888100,\n",
        "    1.101400, 1.022200, 0.814100, 0.886500, 1.123000, 0.851100, 1.182700,\n",
        "    0.875400, 0.958200, 0.866900, 0.984500, 0.892200, 0.760800, 1.130800,\n",
        "    1.134600, 0.530900, 1.067400, 1.509600, 0.795700, 1.226700, 1.274100,\n",
        "    0.854800, 0.995400, 0.865100, 1.057700\n",
        "]\n",
        "\n",
        "# The following are the loss values for 60 epochs of training Llama 3.2 1B with the no changes to the model.\n",
        "\n",
        "# Colab Link: https://colab.research.google.com/drive/1cSNoqCUD95f4aEyKOrFDPm1UQ6xl2y-1?usp=sharing\n",
        "# Github Link: https://github.com/RohitNagraj/UnslothPuzzles/blob/main/resources/q5/Llama3_2_1B_standard.ipynb\n",
        "\n",
        "llama32_1b_training_loss_standard = [\n",
        "    0.877800, 0.965500, 1.159700, 1.032700, 0.826000, 1.079600, 0.750400,\n",
        "    1.135000, 1.015500, 0.884800, 1.010100, 1.255500, 1.073200, 0.745800,\n",
        "    1.009500, 0.740900, 1.143400, 0.983700, 0.885700, 1.014700, 1.005600,\n",
        "    0.987000, 1.146500, 0.995800, 0.758200, 0.942400, 0.965500, 0.902200,\n",
        "    1.254400, 1.179900, 0.818600, 0.631500, 0.752300, 0.700100, 0.888100,\n",
        "    1.101600, 1.022200, 0.814100, 0.886400, 1.123100, 0.851100, 1.182700,\n",
        "    0.875400, 0.958200, 0.866900, 0.984400, 0.892300, 0.760800, 1.130900,\n",
        "    1.134600, 0.530800, 1.067300, 1.509600, 0.795600, 1.226700, 1.274000,\n",
        "    0.854800, 0.995300, 0.865100, 1.057800\n",
        "]"
      ],
      "metadata": {
        "id": "_ZrJogDLNwHk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "steps1 = list(range(1, len(llama32_1b_training_loss_memory_efficient) + 1))\n",
        "steps2 = list(range(1, len(llama32_1b_training_loss_standard) + 1))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps1, llama32_1b_training_loss_memory_efficient, marker='o', label='Memory Efficient Linear Layer')\n",
        "plt.plot(steps2, llama32_1b_training_loss_standard, marker='s', label='Standard Linear Layer')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "t44GKaZAN1zy",
        "outputId": "755c3d56-151a-48d8-a1a8-54662aaa3f73"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observe that the lines are exactly matching, meaning the losses are the exact same.**"
      ],
      "metadata": {
        "id": "3CKeHO77YZ2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRPO Memory Efficient Linear\n",
        "I replaced the lm head in the Unsloth GRPO training notebook and ran the training for 1 iteration. Then compared the output logits for fake input data, using torch.allclose."
      ],
      "metadata": {
        "id": "CLg-Lx4GPLZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory efficient implementation logits\n",
        "\n",
        "# Colab: https://colab.research.google.com/drive/1W6ybjwvgdm3zUrq3jFD54tFJOA-lfIcz?usp=sharing\n",
        "# Github: https://github.com/RohitNagraj/UnslothPuzzles/blob/main/resources/q5/Llama_3_1_8B_GRPO_Memory_Efficient.ipynb\n",
        "\n",
        "memory_efficient_logits = torch.load('resources/q5/logits_memory_efficient.pt', weights_only=False)\n",
        "\n",
        "# Standard implementation logits\n",
        "\n",
        "# Colab: https://colab.research.google.com/drive/1WvcrYgtNubYCB1S-EJg4qJEB5ysJ7K3m?usp=sharing\n",
        "# Github: https://github.com/RohitNagraj/UnslothPuzzles/blob/main/resources/q5/Llama_3_1_8B_GRPO_Standard.ipynb\n",
        "\n",
        "standard_logits = torch.load('resources/q5/logits_standard.pt', weights_only=False)\n",
        "\n",
        "print(\"Torch.allclose: \", torch.allclose(memory_efficient_logits.logits, standard_logits.logits))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SpkJ9wyPKpY",
        "outputId": "0771c70a-a431-4355-86a8-0f7657531e29"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch.allclose:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring():\n",
        "    global final_score\n",
        "    E_score = 0\n",
        "\n",
        "    print(f\"‚úÖ VRAM 50% reduction: E_score += 2\")\n",
        "    E_score += 2\n",
        "\n",
        "    print(f\"‚úÖ Show cross-entropy loss works: E_score += 1\")\n",
        "    E_score += 1\n",
        "\n",
        "    print(f\"‚úÖ Show other loss functions works: E_score += 1\")\n",
        "    E_score += 1\n",
        "\n",
        "    print(f\"‚úÖ Allow dynamic chunk sizes (See the forward definition of MemoryEfficientLinear): E_score += 1\")\n",
        "    E_score += 1\n",
        "\n",
        "    print(f\"‚úÖ Llama 3.2 1B training loss values match across all steps: E_score += 1\")\n",
        "    E_score += 1\n",
        "\n",
        "    print(f\"‚ùå GRPO memory efficient loss working: E_score += 4\")\n",
        "    # E_score += 4\n",
        "\n",
        "    final_score += E_score\n",
        "\n",
        "    print(f\"Final score: {final_score}\")\n",
        "scoring()"
      ],
      "metadata": {
        "id": "XFtBQWwD-SrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb4920f8-e0d5-418b-df52-4b959325c2ea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ VRAM 50% reduction: E_score += 2\n",
            "‚úÖ Show cross-entropy loss works: E_score += 1\n",
            "‚úÖ Show other loss functions works: E_score += 1\n",
            "‚úÖ Allow dynamic chunk sizes (See the forward definition of MemoryEfficientLinear): E_score += 1\n",
            "‚úÖ Llama 3.2 1B training loss values match across all steps: E_score += 1\n",
            "‚ùå GRPO memory efficient loss working: E_score += 4\n",
            "Final score: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVZ414R2Dk8M"
      },
      "source": [
        "## Marking Criteria for E) Max points = 10\n",
        "```python\n",
        "if attemped_E:\n",
        "    E_score = 0\n",
        "    if VRAM_50_percent_reduction: E_score += 2\n",
        "    if remove_float32_upcast: E_score = 0\n",
        "    if show_ce_loss_works: E_score += 1\n",
        "    if show_other_functions_work: E_score += 1\n",
        "    if hardcoded_gradients: E_score = 0\n",
        "    if allows_dynamic_chunk_sizes: E_score += 1\n",
        "    if llama_1B_training_loss_matches: E_score += 1\n",
        "    else: E_score = 0\n",
        "    if GRPO_memory_efficient_linear_works: E_score += 4\n",
        "    final_score += E_score\n",
        "else:\n",
        "    final_score += 0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvrVlTmUN8nV"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"SUBMISSION\"></a>\n",
        "## Submission Steps\n",
        "\n",
        "1. All code should be in a public Github (Apache 2 Licensed)\n",
        "2. Kaggle notebooks and Colab notebooks should be linked in the README, and can be accessible through Colab / Kaggle.\n",
        "3. If attaching notebooks, must attach fully run ones - do not just add a notebook without running it. Kaggle notebook must be public, and run.\n",
        "4. Submit the Github to https://forms.gle/crSYnsGq3t1ck5TB9 If you want to send a private repo, please add me as a Github collaborate @danielhanchen\n",
        "5. Provide screenshots, graphs, plots, etc especially for training loss curves.\n",
        "6. We will comment and respond inside your Github repo. There will get 1 interview as well as a final step!\n",
        "\n",
        "### Clarifications:\n",
        "1. We'll compensate you if we interview you but don't hire you\n",
        "2. \\$100-\\$1000 bounties for Task 4\n",
        "3. Submissions must be Apache-2 licensed\n",
        "4. Task 4 involves solving Github issues for OSS Unsloth\n",
        "5. No time limit: rolling basis\n",
        "6. US based preferred"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}